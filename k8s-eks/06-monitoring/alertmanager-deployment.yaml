apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m

    route:
      group_by: ['alertname', 'severity']
      group_wait: 5s
      group_interval: 45s
      repeat_interval: 20m
      receiver: 'slack-default'
      routes:
      # Critical alerts (immediate, repeat every 20m)
      - match:
          severity: critical
        receiver: 'slack-critical'
        repeat_interval: 20m
        
      # Warning alerts (repeat every 20m)
      - match:
          severity: warning
        receiver: 'slack-warning'
        repeat_interval: 20m

    receivers:
    # Default alerts
    - name: 'slack-default'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/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'
        title: 'üîî Hippo Project Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        send_resolved: true

    # Critical alerts
    - name: 'slack-critical'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/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'
        title: 'üö® CRITICAL: Hippo Project Service Down'
        text: |
          {{ range .Alerts }}
          üî¥ **{{ .Annotations.summary }}**
          
          üìä **Details**:
          ‚Ä¢ Service: `{{ .Labels.job }}`
          ‚Ä¢ Instance: `{{ .Labels.instance }}`
          ‚Ä¢ Started: {{ .StartsAt }}
          ‚Ä¢ Severity: **CRITICAL** üî¥
          ‚Ä¢ Graph: {{ .GeneratorURL }}
          
          üõ†Ô∏è **Recommended Actions**:
          {{ if eq .Labels.alertname "PodDown" }}
          **Service Recovery**:
          1. Check container status: `kubectl get pods -n monitoring`
          2. Restart pod: `kubectl delete pod {{ .Labels.pod }} -n monitoring`
          3. Check logs: `kubectl logs {{ .Labels.pod }} -n monitoring --tail=50`
          4. Check service: `kubectl get svc -n monitoring`
          5. Grafana dashboard: http://k8s-monitoringpublic-28022d4d07-1948928081.ap-northeast-2.elb.amazonaws.com/grafana
          {{ else if eq .Labels.alertname "HighErrorRate" }}
          **API Error Rate Spike Response**:
          1. Check error logs: `kubectl logs {{ .Labels.pod }} -n monitoring --tail=100 | grep -i error`
          2. Check API endpoint: `curl -I http://{{ .Labels.instance }}/health`
          3. Check database: `kubectl logs -l app=postgres -n hippo-project --tail=20`
          4. Prometheus metrics: http://k8s-monitoringpublic-28022d4d07-1948928081.ap-northeast-2.elb.amazonaws.com/prometheus
          5. Restart service: `kubectl rollout restart deployment {{ .Labels.job }} -n monitoring`
          {{ else }}
          **General CRITICAL Response**:
          1. Restart service: `kubectl rollout restart deployment {{ .Labels.job }} -n monitoring`
          2. Check logs: `kubectl logs {{ .Labels.pod }} -n monitoring --tail=50`
          3. Check status: `kubectl get pods -l app={{ .Labels.job }} -n monitoring`
          4. Grafana dashboard: http://k8s-monitoringpublic-28022d4d07-1948928081.ap-northeast-2.elb.amazonaws.com/grafana
          {{ end }}
          {{ end }}
        color: 'danger'
        send_resolved: true

    # Warning alerts
    - name: 'slack-warning'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/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'
        title: '‚ö†Ô∏è WARNING: Hippo Project Performance Issue'
        text: |
          {{ range .Alerts }}
          üü° **{{ .Annotations.summary }}**
          
          üìä **Details**:
          ‚Ä¢ Service: `{{ .Labels.job }}`
          ‚Ä¢ Instance: `{{ .Labels.instance }}`
          ‚Ä¢ Started: {{ .StartsAt }}
          ‚Ä¢ Issue: {{ .Annotations.description }}
          ‚Ä¢ Current Value: **{{ .Value }}**
          ‚Ä¢ Severity: **WARNING** üü°
          ‚Ä¢ Graph: {{ .GeneratorURL }}
          
          üõ†Ô∏è **Recommended Actions**:
          {{ if eq .Labels.alertname "HighMemoryUsage" }}
          **Memory Usage Optimization**:
          1. Check memory usage: `kubectl top pod {{ .Labels.pod }} -n monitoring`
          2. Check container processes: `kubectl exec {{ .Labels.pod }} -n monitoring -- ps aux --sort=-%mem`
          3. Check memory logs: `kubectl logs {{ .Labels.pod }} -n monitoring --tail=100 | grep -i memory`
          4. Grafana memory dashboard: http://k8s-monitoringpublic-28022d4d07-1948928081.ap-northeast-2.elb.amazonaws.com/grafana
          5. Restart if needed: `kubectl rollout restart deployment {{ .Labels.job }} -n monitoring`
          {{ else if eq .Labels.alertname "HighResponseTime" }}
          **Response Time Optimization**:
          1. Check API response time: `curl -w "@curl-format.txt" -o /dev/null -s http://{{ .Labels.instance }}/health`
          2. Check database performance: `kubectl logs -l app=postgres -n hippo-project --tail=50 | grep -i slow`
          3. Check network latency: `kubectl exec {{ .Labels.pod }} -n monitoring -- ping -c 3 8.8.8.8`
          4. Prometheus response time metrics: http://k8s-monitoringpublic-28022d4d07-1948928081.ap-northeast-2.elb.amazonaws.com/prometheus
          5. Grafana response time dashboard: http://k8s-monitoringpublic-28022d4d07-1948928081.ap-northeast-2.elb.amazonaws.com/grafana
          {{ else }}
          **General WARNING Response**:
          1. Check service logs: `kubectl logs {{ .Labels.pod }} -n monitoring --tail=50`
          2. Check resource usage: `kubectl top pod {{ .Labels.pod }} -n monitoring`
          3. Grafana dashboard: http://k8s-monitoringpublic-28022d4d07-1948928081.ap-northeast-2.elb.amazonaws.com/grafana
          4. Prometheus metrics: http://k8s-monitoringpublic-28022d4d07-1948928081.ap-northeast-2.elb.amazonaws.com/prometheus
          {{ end }}
          {{ end }}
        color: 'warning'
        send_resolved: true


---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:latest
        args:
          - '--config.file=/etc/alertmanager/alertmanager.yml'
          - '--storage.path=/alertmanager'
          - '--web.external-url=http://localhost:9093'
        ports:
        - containerPort: 9093
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - name: alertmanager-config-volume
          mountPath: /etc/alertmanager/
        - name: alertmanager-storage-volume
          mountPath: /alertmanager
      volumes:
      - name: alertmanager-config-volume
        configMap:
          defaultMode: 420
          name: alertmanager-config
      - name: alertmanager-storage-volume
        emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  selector:
    app: alertmanager
  ports:
  - port: 9093
    targetPort: 9093
  type: ClusterIP




